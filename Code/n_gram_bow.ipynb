{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing all libraries required\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import nltk, re, pprint, sys\n",
    "from nltk.tokenize import *\n",
    "from nltk import  word_tokenize, pos_tag, wordpunct_tokenize\n",
    "from nltk.collocations import *\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import json\n",
    "from collections import defaultdict,Counter\n",
    "from pprint import pprint\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn as sk\n",
    "from itertools import product\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "with open('dev-v1.1.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paragraphs in train set :  2067\n",
      "Number of question answer pairs in train set :  10570\n",
      "Question type: what \tCount: 5895\n",
      "Question type: why \tCount: 158\n",
      "Question type: when \tCount: 857\n",
      "Question type: where \tCount: 499\n",
      "Question type: how \tCount: 1241\n",
      "Question type: which \tCount: 748\n",
      "Question type: who \tCount: 1280\n",
      "Question type: whom \tCount: 23\n"
     ]
    }
   ],
   "source": [
    "#Number and type of questions in the dataset\n",
    "num_questions=0\n",
    "num_paras=0\n",
    "question_types_count={'what':0, 'why':0, 'when':0,'where':0, 'how':0, 'which':0,'who':0, 'whom':0}\n",
    "for i in range(len(data['data'])):\n",
    "    paragraphs  = data['data'][i]['paragraphs']\n",
    "    qs =[]\n",
    "    for p in paragraphs:  \n",
    "        questions = p['qas']\n",
    "        for q in questions:\n",
    "            qs.append(q['question'])\n",
    "    num_questions += len (qs)\n",
    "    num_paras += len(paragraphs)\n",
    "    question_words = ['what', 'why', 'when','where', 'how', 'which','who', 'whom']\n",
    "    all_qs = \" \".join(q.lower() for q in qs)\n",
    "    word_count = Counter(all_qs.split())\n",
    "    for q in question_words:\n",
    "        question_types_count[q]+=word_count[q]\n",
    "print ('Total number of paragraphs in train set : ',num_paras)      \n",
    "print ('Number of question answer pairs in train set : ',num_questions)\n",
    "for ques,count in question_types_count.items():\n",
    "    print(\"Question type:\",ques,\"\\tCount:\",count  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data processing functions\n",
    "def iter_ngrams(doc, n):\n",
    "    \"\"\"Return a generator over ngrams of a document.\n",
    "    Params:\n",
    "      doc...list of tokens\n",
    "      n.....size of ngrams\"\"\"\n",
    "    return (doc[i: i + n] for i in range(len(doc) - n + 1))\n",
    "\n",
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    #print(sentences)\n",
    "    words = [re.findall('\\w+', sent.lower()) for sent in sentences]\n",
    "    tags = [nltk.pos_tag(word) for word in words]\n",
    "    s = []\n",
    "    t = []\n",
    "    for tag in tags:\n",
    "        s.append([a[0].lower() for a in tag])\n",
    "        t.append([a[1] for a in tag])\n",
    "    return s,t\n",
    "def get_NER(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    s = []\n",
    "    t = []\n",
    "    n = []\n",
    "    for sentence in sentences: \n",
    "        ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        iob_tagged = tree2conlltags(ne_tree)\n",
    "        s.append([each[0] for each in iob_tagged])\n",
    "        t.append([each[1] for each in iob_tagged])\n",
    "        n.append([each[2] for each in iob_tagged])\n",
    "    return s,t,n,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rule based question answer \n",
    "questotag={\"when\":'CD',\n",
    "           \"who\":['B-PERSON','I-PERSON'],\n",
    "           \"whom\":['B-PERSON','I-PERSON'],\n",
    "           \"where\":['B-GPE','I-GPE','B-ORGANIZATION','I-ORGANIZATION'],\n",
    "           \"what\":['B-PERSON','I-PERSON','B-ORGANIZATION','I-ORGANIZATION'],\n",
    "           \"what place\":['B-GPE','I-GPE','B-ORGANIZATION','I-ORGANIZATION'],\n",
    "           \"how many\":\"CD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions to process paragraphs and sentences\n",
    "def get_paras(topic, category):\n",
    "    para_details=[]\n",
    "    paragraphs  = topic['paragraphs'] \n",
    "    for p in paragraphs:\n",
    "        cat_qs=[]\n",
    "        cat_ans=[]\n",
    "        text = p['context']\n",
    "        questions = p['qas']\n",
    "        for q in questions:\n",
    "            if category in (q['question']).lower()[:5]:\n",
    "                cat_qs.append(q['question'])\n",
    "                cat_ans.append(q['answers'][0]['text'])\n",
    "        \n",
    "        if (len(cat_qs)>0):\n",
    "            para_details.append((text,cat_qs,cat_ans))\n",
    "    return para_details\n",
    "\n",
    "def get_ans_sentences(context,question):\n",
    "    ques = re.findall('\\w+', question[:-1].lower())\n",
    "    q = [tuple(ngram) for ngram in iter_ngrams(ques, 1)]\n",
    "    \n",
    "    n_grams =[]\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    for d in sentences:\n",
    "        d = re.findall('\\w+', d.lower())\n",
    "        n_grams.append([tuple(ng) for ng in iter_ngrams(d, 1)])\n",
    "    possible_sentence=[]    \n",
    "    \n",
    "    for indx,t in enumerate(n_grams):\n",
    "        if(len(set.intersection(set(q),set(t))))>=2:\n",
    "            possible_sentence.append(indx)\n",
    "\n",
    "    return possible_sentence\n",
    "\n",
    "def print_possible_answers(para, sent_indices, category):\n",
    "    atype = questotag[category]\n",
    "    sentences,tags,ner,sentece = get_NER(para)\n",
    "    predicted_answers =[]\n",
    "    if len(sent_indices)>0:\n",
    "        for each in (sent_indices):\n",
    "            if(category in ['when','how']):\n",
    "                for t in zip(tags[each],sentences[each]):\n",
    "                    if(t[0] in atype):\n",
    "                        predicted_answers.append(t[1])\n",
    "            else:\n",
    "                for t in zip(ner[each],sentences[each]):\n",
    "                    if(t[0] in atype):\n",
    "                        predicted_answers.append(t[1])\n",
    "    return predicted_answers\n",
    "\n",
    "def print_possible_sentence(para, sent_indices, category):\n",
    "    atype = questotag[category]\n",
    "    sentences,tags,ner,sentece = get_NER(para)\n",
    "    predicted_sentences =[]\n",
    "    if len(sent_indices)>0:\n",
    "        for each in (sent_indices):\n",
    "            if(category in ['when','how']):\n",
    "                for t in zip(tags[each],sentences[each]):\n",
    "                    #print(sentences[each])\n",
    "                    if(t[0] in atype):\n",
    "                        predicted_sentences.append(\" \".join(sentences[each]))\n",
    "            else:\n",
    "                for t in zip(ner[each],sentences[each]):\n",
    "                    #print(sentences[each])\n",
    "                    if(t[0] in atype):\n",
    "                        predicted_sentences.append(\" \".join(sentences[each]))\n",
    "        return predicted_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_train_data_possible_answers():\n",
    "    category = questotag.keys()\n",
    "    data2 = {}\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    topics = data['data']\n",
    "    for t in topics: \n",
    "        true_answer_cnt=0\n",
    "        total_cnt=0\n",
    "        for cat in category:\n",
    "            data2[cat] = get_paras(t, cat)\n",
    "        for cat in category:\n",
    "            for i in range(0,len(data2[cat])):\n",
    "                para = data2[cat][i][0]\n",
    "                question = data2[cat][i][1][0]\n",
    "                sent_indices = get_ans_sentences(para, question)\n",
    "                sent_indices\n",
    "                predicted_answers = print_possible_answers(para,sent_indices, cat)\n",
    "                answer =  data2[cat][i][2][0]\n",
    "                total_cnt+=1\n",
    "                total1+=1\n",
    "                for each in answer.split():\n",
    "                    if(each in predicted_answers):\n",
    "                        true_answer_cnt+=1\n",
    "                        total2+=1\n",
    "                        break\n",
    "        if(total_cnt>0):\n",
    "            print(\"number of answers matched with possible answers\",true_answer_cnt,\"from total of\",total_cnt)\n",
    "            print(\"Accuracy: \",true_answer_cnt/total_cnt)\n",
    "    print(\"number of answers matched with possible answers\",total2,\"from total of\",total1)\n",
    "    print(\"Accuracy: \",total2/total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of answers matched with possible answers 72 from total of 117\n",
      "Accuracy:  0.6153846153846154\n",
      "number of answers matched with possible answers 49 from total of 100\n",
      "Accuracy:  0.49\n",
      "number of answers matched with possible answers 46 from total of 77\n",
      "Accuracy:  0.5974025974025974\n",
      "number of answers matched with possible answers 87 from total of 190\n",
      "Accuracy:  0.45789473684210524\n",
      "number of answers matched with possible answers 9 from total of 51\n",
      "Accuracy:  0.17647058823529413\n",
      "number of answers matched with possible answers 14 from total of 92\n",
      "Accuracy:  0.15217391304347827\n",
      "number of answers matched with possible answers 113 from total of 207\n",
      "Accuracy:  0.5458937198067633\n",
      "number of answers matched with possible answers 16 from total of 37\n",
      "Accuracy:  0.43243243243243246\n",
      "number of answers matched with possible answers 22 from total of 41\n",
      "Accuracy:  0.5365853658536586\n",
      "number of answers matched with possible answers 17 from total of 43\n",
      "Accuracy:  0.3953488372093023\n",
      "number of answers matched with possible answers 50 from total of 90\n",
      "Accuracy:  0.5555555555555556\n",
      "number of answers matched with possible answers 18 from total of 61\n",
      "Accuracy:  0.29508196721311475\n",
      "number of answers matched with possible answers 10 from total of 49\n",
      "Accuracy:  0.20408163265306123\n",
      "number of answers matched with possible answers 16 from total of 31\n",
      "Accuracy:  0.5161290322580645\n",
      "number of answers matched with possible answers 46 from total of 86\n",
      "Accuracy:  0.5348837209302325\n",
      "number of answers matched with possible answers 25 from total of 58\n",
      "Accuracy:  0.43103448275862066\n",
      "number of answers matched with possible answers 7 from total of 28\n",
      "Accuracy:  0.25\n",
      "number of answers matched with possible answers 10 from total of 43\n",
      "Accuracy:  0.23255813953488372\n",
      "number of answers matched with possible answers 21 from total of 39\n",
      "Accuracy:  0.5384615384615384\n",
      "number of answers matched with possible answers 9 from total of 30\n",
      "Accuracy:  0.3\n",
      "number of answers matched with possible answers 19 from total of 47\n",
      "Accuracy:  0.40425531914893614\n",
      "number of answers matched with possible answers 7 from total of 35\n",
      "Accuracy:  0.2\n",
      "number of answers matched with possible answers 45 from total of 87\n",
      "Accuracy:  0.5172413793103449\n",
      "number of answers matched with possible answers 43 from total of 79\n",
      "Accuracy:  0.5443037974683544\n",
      "number of answers matched with possible answers 118 from total of 161\n",
      "Accuracy:  0.7329192546583851\n",
      "number of answers matched with possible answers 48 from total of 96\n",
      "Accuracy:  0.5\n",
      "number of answers matched with possible answers 8 from total of 46\n",
      "Accuracy:  0.17391304347826086\n",
      "number of answers matched with possible answers 12 from total of 55\n",
      "Accuracy:  0.21818181818181817\n",
      "number of answers matched with possible answers 11 from total of 43\n",
      "Accuracy:  0.2558139534883721\n",
      "number of answers matched with possible answers 0 from total of 28\n",
      "Accuracy:  0.0\n",
      "number of answers matched with possible answers 8 from total of 31\n",
      "Accuracy:  0.25806451612903225\n",
      "number of answers matched with possible answers 17 from total of 32\n",
      "Accuracy:  0.53125\n",
      "number of answers matched with possible answers 10 from total of 29\n",
      "Accuracy:  0.3448275862068966\n",
      "number of answers matched with possible answers 16 from total of 73\n",
      "Accuracy:  0.2191780821917808\n",
      "number of answers matched with possible answers 68 from total of 112\n",
      "Accuracy:  0.6071428571428571\n",
      "number of answers matched with possible answers 45 from total of 60\n",
      "Accuracy:  0.75\n",
      "number of answers matched with possible answers 57 from total of 103\n",
      "Accuracy:  0.5533980582524272\n",
      "number of answers matched with possible answers 46 from total of 101\n",
      "Accuracy:  0.45544554455445546\n",
      "number of answers matched with possible answers 23 from total of 44\n",
      "Accuracy:  0.5227272727272727\n",
      "number of answers matched with possible answers 9 from total of 92\n",
      "Accuracy:  0.09782608695652174\n",
      "number of answers matched with possible answers 7 from total of 39\n",
      "Accuracy:  0.1794871794871795\n",
      "number of answers matched with possible answers 41 from total of 85\n",
      "Accuracy:  0.4823529411764706\n",
      "number of answers matched with possible answers 23 from total of 78\n",
      "Accuracy:  0.2948717948717949\n",
      "number of answers matched with possible answers 44 from total of 88\n",
      "Accuracy:  0.5\n",
      "number of answers matched with possible answers 18 from total of 66\n",
      "Accuracy:  0.2727272727272727\n",
      "number of answers matched with possible answers 45 from total of 65\n",
      "Accuracy:  0.6923076923076923\n",
      "number of answers matched with possible answers 53 from total of 85\n",
      "Accuracy:  0.6235294117647059\n",
      "number of answers matched with possible answers 17 from total of 65\n",
      "Accuracy:  0.26153846153846155\n",
      "number of answers matched with possible answers 1515 from total of 3395\n",
      "Accuracy:  0.44624447717231225\n"
     ]
    }
   ],
   "source": [
    "process_train_data_possible_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_train_data():\n",
    "    category = questotag.keys()\n",
    "    data2 = {}\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    topics = data['data']\n",
    "    with open(\"Question_answer\",\"wb\") as f:\n",
    "        for t in topics: \n",
    "            true_answer_cnt=0\n",
    "            total_cnt=0\n",
    "            for cat in category:\n",
    "                data2[cat] = get_paras(t, cat)\n",
    "            for cat in category:\n",
    "                for i in range(0,len(data2[cat])):\n",
    "                    para = data2[cat][i][0]\n",
    "                    question = data2[cat][i][1][0]\n",
    "                    sent_indices = get_ans_sentences(para, question)\n",
    "                    sent_indices\n",
    "                    predicted_sentence = print_possible_sentence(para,sent_indices, cat)\n",
    "                    answer =  data2[cat][i][2][0]\n",
    "                    total_cnt+=1\n",
    "                    total1+=1\n",
    "                    cnd=0\n",
    "                    if(predicted_sentence!=None):\n",
    "                        for i in predicted_sentence:\n",
    "                            if answer in i:\n",
    "                                true_answer_cnt+=1\n",
    "                                total2+=1\n",
    "                                pickle.dump((question+\" \"+i,1),f)\n",
    "                                cnd=1\n",
    "                                break\n",
    "                            else:\n",
    "                                pickle.dump((question+\" \"+i,0),f)  \n",
    "                    if(cnd==0):\n",
    "                        pickle.dump((question+\" \"+answer,1),f)\n",
    "    print(\"number of answers matched with possible answers\",total2,\"from total of\",total1)\n",
    "    print(\"Accuracy: \",total2/total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "    with open('dev-v1.1.json') as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    category = questotag.keys()\n",
    "    data2 = {}\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    topics = data['data']\n",
    "    with open(\"Question_answer_test\",\"wb\") as f:\n",
    "        for t in topics: \n",
    "            true_answer_cnt=0\n",
    "            total_cnt=0\n",
    "            for cat in category:\n",
    "                data2[cat] = get_paras(t, cat)\n",
    "            for cat in category:\n",
    "                for i in range(0,len(data2[cat])):\n",
    "                    para = data2[cat][i][0]\n",
    "                    question = data2[cat][i][1][0]\n",
    "                    sent_indices = get_ans_sentences(para, question)\n",
    "                    sent_indices\n",
    "                    predicted_sentence = print_possible_sentence(para,sent_indices, cat)\n",
    "                    answer =  data2[cat][i][2][0]\n",
    "                    total_cnt+=1\n",
    "                    total1+=1\n",
    "                    cnd=0\n",
    "                    if(predicted_sentence!=None):\n",
    "                        for i in predicted_sentence:\n",
    "                            if answer in i:\n",
    "                                true_answer_cnt+=1\n",
    "                                total2+=1\n",
    "                                pickle.dump((question+\" \"+i,1),f)\n",
    "                                cnd=1\n",
    "                                break\n",
    "                            else:\n",
    "                                pickle.dump((question+\" \"+i,0),f)\n",
    "                    if(cnd==0):\n",
    "                        pickle.dump((question+\" \"+answer,1),f)\n",
    "    print(\"number of answers matched with possible answers\",total2,\"from total of\",total1)\n",
    "    print(\"Accuracy: \",total2/total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of answers matched with possible answers 2099 from total of 3395\n",
      "Accuracy:  0.6182621502209131\n",
      "number of answers matched with possible answers 2099 from total of 3395\n",
      "Accuracy:  0.6182621502209131\n"
     ]
    }
   ],
   "source": [
    "#Run only if you want to change the \"Question_answer\" and  \"Question_answer_test\" file\n",
    "process_train_data()\n",
    "process_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_test():\n",
    "    objs=[]\n",
    "    with open(\"Question_answer\",\"rb\") as f:\n",
    "        while 1:\n",
    "            try:\n",
    "                objs.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for o in objs:\n",
    "        X.append(o[0])\n",
    "        Y.append(o[1])\n",
    "    vec = TfidfVectorizer()\n",
    "    X_csr = vec.fit_transform(X)\n",
    "    objs=[]\n",
    "    with open(\"Question_answer_test_2\",\"rb\") as f:\n",
    "        while 1:\n",
    "            try:\n",
    "                objs.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    X_val=X_csr[-100:,:]\n",
    "    Y_val=Y[-100:]\n",
    "    X_csr=X_csr[:-100,:]\n",
    "    Y=Y[:-100]\n",
    "    X_test = []\n",
    "    Y_test= []\n",
    "    for o in objs:\n",
    "        X_test.append(o[0])\n",
    "        Y_test.append(o[1])\n",
    "    X_test_csr = vec.transform(X_test)\n",
    "    return X_csr,Y,X_val,Y_val,X_test_csr,Y_test,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tunning_parameters(X,Y,X_test,y_test):\n",
    "    tuned_parameters = [{'random_state': [2,42,None], \n",
    "                         'penalty': ['l1', 'l2'],\n",
    "                         'C': [1,0.1],\n",
    "                         'class_weight': ['balanced',None]}]\n",
    "    scores = ['precision', 'recall','f1']\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "        clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv=5,\n",
    "                           scoring='%s_macro' % score)\n",
    "        clf.fit(X,y)\n",
    "        print(\"Best parameters set found on validation set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print(\"Grid scores on validation set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is evaluated on the full validation set.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(sk.metrics.classification_report(y_true, y_pred))\n",
    "    return clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "Grid scores on development set:\n",
      "\n",
      "0.616 (+/-0.032) for {'C': 1, 'random_state': 2, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.616 (+/-0.032) for {'C': 1, 'random_state': 42, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.616 (+/-0.032) for {'C': 1, 'random_state': None, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.623 (+/-0.030) for {'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.623 (+/-0.030) for {'C': 1, 'random_state': 42, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.623 (+/-0.030) for {'C': 1, 'random_state': None, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.655 (+/-0.036) for {'C': 1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "0.655 (+/-0.036) for {'C': 1, 'random_state': 42, 'penalty': 'l1', 'class_weight': None}\n",
      "0.655 (+/-0.036) for {'C': 1, 'random_state': None, 'penalty': 'l1', 'class_weight': None}\n",
      "0.662 (+/-0.024) for {'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "0.662 (+/-0.024) for {'C': 1, 'random_state': 42, 'penalty': 'l2', 'class_weight': None}\n",
      "0.662 (+/-0.024) for {'C': 1, 'random_state': None, 'penalty': 'l2', 'class_weight': None}\n",
      "0.647 (+/-0.019) for {'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.647 (+/-0.019) for {'C': 0.1, 'random_state': 42, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.647 (+/-0.019) for {'C': 0.1, 'random_state': None, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.645 (+/-0.020) for {'C': 0.1, 'random_state': 2, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.645 (+/-0.020) for {'C': 0.1, 'random_state': 42, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.645 (+/-0.020) for {'C': 0.1, 'random_state': None, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.748 (+/-0.038) for {'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "0.748 (+/-0.038) for {'C': 0.1, 'random_state': 42, 'penalty': 'l1', 'class_weight': None}\n",
      "0.748 (+/-0.038) for {'C': 0.1, 'random_state': None, 'penalty': 'l1', 'class_weight': None}\n",
      "0.745 (+/-0.033) for {'C': 0.1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "0.745 (+/-0.033) for {'C': 0.1, 'random_state': 42, 'penalty': 'l2', 'class_weight': None}\n",
      "0.745 (+/-0.033) for {'C': 0.1, 'random_state': None, 'penalty': 'l2', 'class_weight': None}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is evaluated on the full validation set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.89      0.90        76\n",
      "          1       0.68      0.71      0.69        24\n",
      "\n",
      "avg / total       0.85      0.85      0.85       100\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "Grid scores on development set:\n",
      "\n",
      "0.630 (+/-0.037) for {'C': 1, 'random_state': 2, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.630 (+/-0.037) for {'C': 1, 'random_state': 42, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.630 (+/-0.037) for {'C': 1, 'random_state': None, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.638 (+/-0.035) for {'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.638 (+/-0.035) for {'C': 1, 'random_state': 42, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.638 (+/-0.035) for {'C': 1, 'random_state': None, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.659 (+/-0.037) for {'C': 1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "0.659 (+/-0.037) for {'C': 1, 'random_state': 42, 'penalty': 'l1', 'class_weight': None}\n",
      "0.659 (+/-0.036) for {'C': 1, 'random_state': None, 'penalty': 'l1', 'class_weight': None}\n",
      "0.666 (+/-0.026) for {'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "0.666 (+/-0.026) for {'C': 1, 'random_state': 42, 'penalty': 'l2', 'class_weight': None}\n",
      "0.666 (+/-0.026) for {'C': 1, 'random_state': None, 'penalty': 'l2', 'class_weight': None}\n",
      "0.665 (+/-0.021) for {'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.665 (+/-0.021) for {'C': 0.1, 'random_state': 42, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.665 (+/-0.021) for {'C': 0.1, 'random_state': None, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.664 (+/-0.024) for {'C': 0.1, 'random_state': 2, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.664 (+/-0.024) for {'C': 0.1, 'random_state': 42, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.664 (+/-0.024) for {'C': 0.1, 'random_state': None, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.660 (+/-0.019) for {'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "0.660 (+/-0.019) for {'C': 0.1, 'random_state': 42, 'penalty': 'l1', 'class_weight': None}\n",
      "0.660 (+/-0.019) for {'C': 0.1, 'random_state': None, 'penalty': 'l1', 'class_weight': None}\n",
      "0.657 (+/-0.023) for {'C': 0.1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "0.657 (+/-0.023) for {'C': 0.1, 'random_state': 42, 'penalty': 'l2', 'class_weight': None}\n",
      "0.657 (+/-0.023) for {'C': 0.1, 'random_state': None, 'penalty': 'l2', 'class_weight': None}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is evaluated on the full validation set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.75      0.82        76\n",
      "          1       0.49      0.75      0.59        24\n",
      "\n",
      "avg / total       0.80      0.75      0.76       100\n",
      "\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "Grid scores on development set:\n",
      "\n",
      "0.590 (+/-0.037) for {'C': 1, 'random_state': 2, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.590 (+/-0.037) for {'C': 1, 'random_state': 42, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.590 (+/-0.037) for {'C': 1, 'random_state': None, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.594 (+/-0.037) for {'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.594 (+/-0.037) for {'C': 1, 'random_state': 42, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.594 (+/-0.037) for {'C': 1, 'random_state': None, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.657 (+/-0.037) for {'C': 1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "0.657 (+/-0.037) for {'C': 1, 'random_state': 42, 'penalty': 'l1', 'class_weight': None}\n",
      "0.657 (+/-0.037) for {'C': 1, 'random_state': None, 'penalty': 'l1', 'class_weight': None}\n",
      "0.663 (+/-0.026) for {'C': 1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "0.663 (+/-0.026) for {'C': 1, 'random_state': 42, 'penalty': 'l2', 'class_weight': None}\n",
      "0.663 (+/-0.026) for {'C': 1, 'random_state': None, 'penalty': 'l2', 'class_weight': None}\n",
      "0.647 (+/-0.021) for {'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.647 (+/-0.021) for {'C': 0.1, 'random_state': 42, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.647 (+/-0.021) for {'C': 0.1, 'random_state': None, 'penalty': 'l1', 'class_weight': 'balanced'}\n",
      "0.636 (+/-0.024) for {'C': 0.1, 'random_state': 2, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.636 (+/-0.024) for {'C': 0.1, 'random_state': 42, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.636 (+/-0.024) for {'C': 0.1, 'random_state': None, 'penalty': 'l2', 'class_weight': 'balanced'}\n",
      "0.672 (+/-0.022) for {'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n",
      "0.672 (+/-0.022) for {'C': 0.1, 'random_state': 42, 'penalty': 'l1', 'class_weight': None}\n",
      "0.672 (+/-0.022) for {'C': 0.1, 'random_state': None, 'penalty': 'l1', 'class_weight': None}\n",
      "0.669 (+/-0.027) for {'C': 0.1, 'random_state': 2, 'penalty': 'l2', 'class_weight': None}\n",
      "0.669 (+/-0.027) for {'C': 0.1, 'random_state': 42, 'penalty': 'l2', 'class_weight': None}\n",
      "0.669 (+/-0.027) for {'C': 0.1, 'random_state': None, 'penalty': 'l2', 'class_weight': None}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is evaluated on the full validation set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.89      0.90        76\n",
      "          1       0.68      0.71      0.69        24\n",
      "\n",
      "avg / total       0.85      0.85      0.85       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, X_val, Y_val, X_test, y_test = create_train_test()\n",
    "result=tunning_parameters(X,y,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'random_state': 2, 'penalty': 'l1', 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "def logistic_model(X_csr,Y,X_val,Y_val,X_test_csr,Y_test,X): \n",
    "    print('Training data shape: %s\\n' % str(X_csr.shape))\n",
    "    clf = LogisticRegression(C= 0.1, random_state=2, penalty='l1', class_weight=None)\n",
    "    clf.fit(X_csr, Y)\n",
    "    acc=cross_val_score(clf,X_csr,Y,cv=10)\n",
    "    print(\"Accuracy on training dataset\",np.mean(acc))\n",
    "    print('Test data shape: %s\\n' % str(X_test_csr.shape))\n",
    "    y_pred=clf.predict(X_test_csr)\n",
    "    print(sk.metrics.classification_report(Y_test,y_pred))\n",
    "    print (\"\\nPrecision\", sk.metrics.precision_score(Y_test, y_pred))\n",
    "    print (\"\\nRecall\", sk.metrics.recall_score(Y_test, y_pred))\n",
    "    print (\"\\nf1_score\", sk.metrics.f1_score(Y_test, y_pred))\n",
    "    print (\"\\nconfusion_matrix\")\n",
    "    print (sk.metrics.confusion_matrix(Y_test, y_pred))\n",
    "    question_types_count={'what':0, 'why':0, 'when':0,'where':0, 'how':0, 'which':0,'who':0, 'whom':0}\n",
    "    question_words = ['what', 'why', 'when','where', 'how', 'which','who', 'whom']\n",
    "    word_count = Counter()\n",
    "    print(Counter(y_pred))\n",
    "    for i,y in enumerate(y_pred):\n",
    "        if y == 1:        \n",
    "            all_qs = X[i].lower()\n",
    "            for q in all_qs.split():\n",
    "                if q in question_types_count.keys():\n",
    "                    question_types_count[q] += 1\n",
    "    print(question_types_count)\n",
    "    word_count.clear()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (87707, 54867)\n",
      "\n",
      "Accuracy on training dataset 0.754113348708\n",
      "Test data shape: (10368, 54867)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.94      0.84      6973\n",
      "          1       0.74      0.39      0.51      3395\n",
      "\n",
      "avg / total       0.75      0.76      0.73     10368\n",
      "\n",
      "\n",
      "Precision 0.744753261486\n",
      "\n",
      "Recall 0.386745213549\n",
      "\n",
      "f1_score 0.509112058938\n",
      "\n",
      "confusion_matrix\n",
      "[[6523  450]\n",
      " [2082 1313]]\n",
      "Counter({0: 8605, 1: 1763})\n",
      "{'where': 193, 'who': 446, 'how': 9, 'which': 159, 'what': 1123, 'why': 13, 'whom': 15, 'when': 225}\n"
     ]
    }
   ],
   "source": [
    "logistic_model(X, y, X_val, Y_val, X_test, y_test, X_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP Classifier\n",
    "def neural_model():\n",
    "    objs=[]\n",
    "    with open(\"Question_answer\",\"rb\") as f:\n",
    "        while 1:\n",
    "            try:\n",
    "                objs.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    X = []\n",
    "    Y= []\n",
    "    for o in objs:\n",
    "        X.append(o[0])\n",
    "        Y.append(o[1])\n",
    "    vec = TfidfVectorizer()\n",
    "    X_csr = vec.fit_transform(X)\n",
    "    print('Training data shape: %s\\n' % str(X_csr.shape))\n",
    "    clf = MLPClassifier(random_state=1,solver=\"lbfgs\", hidden_layer_sizes=(5,2))\n",
    "    clf.fit(X_csr, Y)\n",
    "    acc=cross_val_score(clf,X_csr,Y,cv=10)\n",
    "    print(\"Accuracy on training dataset\",np.mean(acc))\n",
    "    objs=[]\n",
    "    with open(\"Question_answer_test_2\",\"rb\") as f:\n",
    "        while 1:\n",
    "            try:\n",
    "                objs.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    X_test = []\n",
    "    Y_test= []\n",
    "    \n",
    "    for o in objs:\n",
    "        X_test.append(o[0])\n",
    "        Y_test.append(o[1])\n",
    "    X_test_csr = vec.transform(X_test)\n",
    "    print('Test data shape: %s\\n' % str(X_test_csr.shape))\n",
    "    y_pred=clf.predict(X_test_csr)\n",
    "    print(sk.metrics.classification_report(Y_test,y_pred))\n",
    "    print (\"\\nPrecision\", sk.metrics.precision_score(Y_test, y_pred))\n",
    "    print (\"\\nRecall\", sk.metrics.recall_score(Y_test, y_pred))\n",
    "    print (\"\\nf1_score\", sk.metrics.f1_score(Y_test, y_pred))\n",
    "    print (\"\\nconfusion_matrix\")\n",
    "    print (sk.metrics.confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (87807, 54867)\n",
      "\n",
      "Accuracy on training dataset 0.562632914719\n",
      "Test data shape: (10368, 54867)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.66      0.72      6973\n",
      "          1       0.48      0.64      0.55      3395\n",
      "\n",
      "avg / total       0.69      0.65      0.66     10368\n",
      "\n",
      "\n",
      "Precision 0.479079034758\n",
      "\n",
      "Recall 0.637407952872\n",
      "\n",
      "f1_score 0.54701718908\n",
      "\n",
      "confusion_matrix\n",
      "[[4620 2353]\n",
      " [1231 2164]]\n"
     ]
    }
   ],
   "source": [
    "neural_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "####### Keras sequential method ##########\n",
    "##########################################\n",
    "def keras_model():\n",
    "    objs=[]\n",
    "    with open(\"Question_answer\",\"rb\") as f:\n",
    "        while 1:\n",
    "            try:\n",
    "                objs.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    X = []\n",
    "    Y= []\n",
    "    for o in objs:\n",
    "        X.append(o[0])\n",
    "        Y.append(o[1])\n",
    "    vec = TfidfVectorizer()\n",
    "    X_csr = vec.fit_transform(X)\n",
    "    X_csr=X_csr.toarray()\n",
    "    print('Training data shape: %s\\n' % str(X_csr.shape))\n",
    "   \n",
    "    Y = to_categorical(Y)\n",
    "    \n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Dense(20,input_dim=X_csr.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    print(model.output_shape)\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    model.fit(X_csr,Y,epochs=2)\n",
    "    objs=[]\n",
    "    score = model.evaluate(X_csr, Y)\n",
    "    y_true=Y_test\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "    \n",
    "    \n",
    "    with open(\"Question_answer_test_2\",\"rb\") as f:\n",
    "        while 1:\n",
    "            try:\n",
    "                objs.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    X_test = []\n",
    "    Y_test= []\n",
    "    \n",
    "    for o in objs:\n",
    "        X_test.append(o[0])\n",
    "        Y_test.append(o[1])\n",
    "    X_test_csr = vec.transform(X_test)\n",
    "    X_test_csr=X_test_csr.toarray()\n",
    "    Y_test = to_categorical(Y_test)\n",
    "    print('Test data shape: %s\\n' % str(X_test_csr.shape))\n",
    "    \n",
    "    loss_metrics=model.evaluate(X_test_csr,Y_test)\n",
    "    classes=model.predict(X_test_csr)\n",
    "    score = model.evaluate(X_test_csr, Y_test)\n",
    "    y_true=Y_test\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "    y_pred=[np.argmax(c) for c in classes]\n",
    "    y_pred = to_categorical(y_pred)\n",
    "    print(Counter([np.argmax(c) for c in classes]))\n",
    "    print(sk.metrics.classification_report(y_true,y_pred))\n",
    "    print (\"\\nPrecision\", sk.metrics.precision_score(y_true, y_pred))\n",
    "    print (\"\\nRecall\", sk.metrics.recall_score(y_true, y_pred))\n",
    "    print (\"\\nf1_score\", sk.metrics.f1_score(y_true, y_pred))\n",
    "    print (\"\\nconfusion_matrix\")\n",
    "    print (sk.metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (87807, 54867)\n",
      "\n",
      "(None, 20)\n",
      "Epoch 1/2\n",
      "87807/87807 [==============================] - 125s - loss: 0.5160 - acc: 0.7520   \n",
      "Epoch 2/2\n",
      "87807/87807 [==============================] - 129s - loss: 0.4534 - acc: 0.7929   \n",
      "Test data shape: (10368, 54867)\n",
      "\n",
      "10336/10368 [============================>.] - ETA: 0s\n",
      "acc: 72.99%\n",
      "Counter({0: 7196, 1: 3172})\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.82      0.80      6973\n",
      "          1       0.59      0.55      0.57      3395\n",
      "\n",
      "avg / total       0.73      0.73      0.73     10368\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multilabel-indicator but average='binary'. Please choose another average setting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aa85d5137d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-cf55bac35dbf>\u001b[0m in \u001b[0;36mkeras_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPrecision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRecall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nf1_score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m-> 1018\u001b[0;31m                              \"choose another average setting.\" % y_type)\n\u001b[0m\u001b[1;32m   1019\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multilabel-indicator but average='binary'. Please choose another average setting."
     ]
    }
   ],
   "source": [
    "keras_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
